OSLGN currently supports only binary logic gates with hard operand selection. This restricts expressivity in tasks requiring multi-bit reasoning or smooth composition. Additionally, the operand selection layers may introduce redundancy: the learned symbolic expressions often use only a subset of the full networkâ€™s capacity, suggesting overparameterization. Another challenge is training stability during early stages, particularly when the routing distributions are uncertain. While softmax-based selection is viable, convergence strategies for stable symbolic gating remain underexplored.
