The core mechanism in OSLGN is argmax-based selection with a Straight-Through Estimator (STE), applied to both operand and operator routing. This allows symbolic consistency and gradient-based learning, despite the discrete nature of the computation. While soft selection methods such as Gumbel-Softmax demonstrated faster convergence in preliminary experiments, they require additional stabilization strategies. Future work may explore advanced routing mechanisms to improve convergence speed and training stability.
