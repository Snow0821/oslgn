Neural-symbolic models aim to integrate logical reasoning into neural networks. Early efforts like DeepProbLog~\cite{manhaeve2018deepproblog} and Logic Tensor Networks~\cite{serafini2016logic} incorporated symbolic logic via probabilistic inference or fuzzy semantics, but required predefined logic templates. Neural Logic Machines~\cite{dong2019nlm} introduced trainable logical operations, yet imposed rigid operand structures.

Logical Neural Networks (LNNs)~\cite{riegel2020lnn} extended this line by learning differentiable representations of logical formulas using fuzzy logic operators at the neuron level. While expressive, their logic is embedded in continuous-valued activations. In contrast, our model composes discrete logic circuits via operand and operator selection, allowing symbolic structure to emerge during training. This enables our system to approximate logic expressions in a form directly symbolizable.
