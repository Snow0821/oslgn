Mixture-of-Experts (MoE) architectures enable conditional computation by activating only a subset of experts per input, enhancing scalability and efficiency. Pioneering works like the Sparsely-Gated MoE~\cite{shazeer2017outrageously} and Switch Transformer~\cite{fedus2022switch} utilize token-level top-k routing. Expert Choice Routing~\cite{zhou2022mixture} reverses this paradigm, allowing experts to select tokens, improving load balancing.

Recent advancements introduce fine-grained routing and sparse masking techniques. DSelect-k~\cite{hazimeh2021dselect} offers differentiable top-k selection without softmax, while BASE Layers~\cite{lewis2021base} employ linear assignment for expert allocation, mitigating expert collapse. Hash Layers~\cite{roller2021hash} provide deterministic routing via hashing functions, eliminating the need for learned gating.

Our approach diverges by implementing operand and operator-level routing within logic gate modules, utilizing argmax with Straight-Through Estimator (STE) for discrete selection. This fine-grained, symbolic routing contrasts with traditional MoE strategies, enabling the construction of symbolic logic circuits within neural networks.
