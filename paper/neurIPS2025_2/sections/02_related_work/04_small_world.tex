Small-world networks, characterized by high clustering and short average path lengths, have been shown to enhance information propagation and convergence in neural networks. Early studies demonstrated that small-world connectivity reduces learning error and accelerates training compared to regular or random networks~\cite{watts1998collective,ramanujan2020s}.

Recent architectures, such as SWNet~\cite{javaheripi2019swnet}, introduce small-world topologies into deep learning models, facilitating gradient flow and feature reuse through long-range connections. These designs improve convergence speed and generalization across various tasks.

Inspired by the efficiency of small-world connectivity, our approach integrates similar principles within logic gate modules. While not implementing a traditional small-world network, we adopt its structural characteristics to enable efficient information flow and modular reasoning. This design choice fosters the emergence of symbolizable logic circuits during training.
