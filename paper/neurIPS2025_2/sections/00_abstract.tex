\begin{abstract}
    Deep neural networks often lack modularity, interpretability, and computational sparsityâ€”properties that are intrinsic to logical systems. Logic gate-based neural networks offer a promising direction by enabling discrete, symbolic computation, but suffer from a critical limitation: they rely on fixed or random operand connectivity, which restricts their flexibility and generalization. In this paper, we propose Operand Selective Logic Gated Neural Networks (OSLGN), a novel architecture that learns to select both operands and logical operations for each unit in the network. We introduce a fully discrete operand selection mechanism based on argmax and straight-through estimation (STE), allowing the model to dynamically route information between units while preserving symbolic structure. Operator selection is likewise handled discretely to maintain consistency between training and inference. Our approach eliminates the need for post-training quantization and enables efficient, interpretable computation. Experiments on MNIST confirm that OSLGN can be trained end-to-end with discrete gates, providing initial evidence of its feasibility and interpretability. Further evaluation on more complex tasks remains as future work.
\end{abstract}