Operand-Selective Logic Gate Networks (OSLGN) represent a step toward unifying deep learning with symbolic reasoning. By learning networks of logic gates through operand and operator selection, OSLGN performs symbolic computation while remaining trainable via gradient-based optimization using the Straight-Through Estimator (STE). We showed that this architecture can be initialized with inductive priors to enhance convergence, trained with discrete routing, and post-hoc translated into compact Boolean expressions.

While classification performance remains limited, the symbolic structure of OSLGN supports modularity and compatibility with digital logic. A key challenge is the relatively large network size required to express symbolic patterns, indicating redundancy or inefficiency in operand routing. Our work lays the groundwork for trainable logic circuits and opens the door for future models that are both compact and symbolic, enabling structured and efficient AI systems.
